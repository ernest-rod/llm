"""
Binary File Reader - Flexible Production Version
================================================
A highly flexible binary file reader for data generated by C programs.
Supports configurable data structures, robust error handling, validation,
and comprehensive diagnostic capabilities.

Features:
- Configurable field schemas for any data structure
- Multiple data type support (integers, floats, strings, booleans)
- Custom validation rules per field
- Multiple error handling modes
- Detailed logging and diagnostics
- Memory-efficient processing
- Character encoding fallback
- File integrity verification

Author: Enhanced Flexible Version
Date: 2025-12-26
Version: 2.0
"""

import struct
import os
import logging
import re
from typing import List, Dict, Optional, Tuple, Any, Union, Callable
from enum import Enum
from dataclasses import dataclass, field
from pathlib import Path
from datetime import datetime


class ErrorHandlingMode(Enum):
    """Defines how the reader handles errors during processing."""
    STRICT = "strict"           # Stop on first error
    SKIP_INVALID = "skip"       # Skip invalid records and continue
    COLLECT_ERRORS = "collect"  # Collect all errors and continue


class FieldType(Enum):
    """Supported field data types."""
    INT8 = "int8"       # signed char
    UINT8 = "uint8"     # unsigned char
    INT16 = "int16"     # short
    UINT16 = "uint16"   # unsigned short
    INT32 = "int32"     # int
    UINT32 = "uint32"   # unsigned int
    INT64 = "int64"     # long long
    UINT64 = "uint64"   # unsigned long long
    FLOAT = "float"     # float
    DOUBLE = "double"   # double
    STRING = "string"   # char array
    BOOL = "bool"       # boolean (as byte)


# Mapping of FieldType to struct format characters
FIELD_TYPE_TO_STRUCT = {
    FieldType.INT8: 'b',
    FieldType.UINT8: 'B',
    FieldType.INT16: 'h',
    FieldType.UINT16: 'H',
    FieldType.INT32: 'i',
    FieldType.UINT32: 'I',
    FieldType.INT64: 'q',
    FieldType.UINT64: 'Q',
    FieldType.FLOAT: 'f',
    FieldType.DOUBLE: 'd',
    FieldType.BOOL: '?',
}


@dataclass
class FieldDefinition:
    """
    Defines a single field in the binary record structure.
    
    Attributes:
        name: Field name (must be unique within schema)
        field_type: Type of the field (from FieldType enum)
        string_length: Length for STRING type fields (required if field_type is STRING)
        description: Optional description of the field
        validator: Optional custom validation function
        required: Whether this field must have a value (default: True)
        default_value: Default value if field is invalid and error mode allows skipping
    """
    name: str
    field_type: FieldType
    string_length: Optional[int] = None
    description: str = ""
    validator: Optional[Callable[[Any], bool]] = None
    required: bool = True
    default_value: Any = None
    
    def __post_init__(self):
        """Validate field definition."""
        if self.field_type == FieldType.STRING and self.string_length is None:
            raise ValueError(f"Field '{self.name}': STRING type requires string_length")
        if self.field_type != FieldType.STRING and self.string_length is not None:
            raise ValueError(f"Field '{self.name}': string_length only valid for STRING type")
    
    def get_struct_format(self) -> str:
        """Returns the struct format string for this field."""
        if self.field_type == FieldType.STRING:
            return f"{self.string_length}s"
        return FIELD_TYPE_TO_STRUCT[self.field_type]
    
    def get_size(self) -> int:
        """Returns the size in bytes of this field."""
        return struct.calcsize(self.get_struct_format())


@dataclass
class RecordSchema:
    """
    Defines the complete structure of a binary record.
    
    Attributes:
        fields: List of FieldDefinition objects in order
        byte_order: Byte order ('< for little-endian, '>' for big-endian, '=' for native)
        name: Optional name for this schema
        description: Optional description of this record structure
    """
    fields: List[FieldDefinition]
    byte_order: str = '<'
    name: str = "Default Schema"
    description: str = ""
    
    def __post_init__(self):
        """Validate schema and check for duplicate field names."""
        if not self.fields:
            raise ValueError("Schema must have at least one field")
        
        # Check for duplicate field names
        field_names = [f.name for f in self.fields]
        if len(field_names) != len(set(field_names)):
            duplicates = [name for name in field_names if field_names.count(name) > 1]
            raise ValueError(f"Duplicate field names in schema: {set(duplicates)}")
        
        # Validate byte order
        if self.byte_order not in ['<', '>', '=', '@', '!']:
            raise ValueError(f"Invalid byte order: {self.byte_order}")
    
    @property
    def format_string(self) -> str:
        """Returns the complete struct format string."""
        format_parts = [field.get_struct_format() for field in self.fields]
        return f"{self.byte_order}{''.join(format_parts)}"
    
    @property
    def record_size(self) -> int:
        """Returns the total size in bytes of one record."""
        return struct.calcsize(self.format_string)
    
    @property
    def field_names(self) -> List[str]:
        """Returns list of field names in order."""
        return [field.name for field in self.fields]
    
    def get_field(self, name: str) -> Optional[FieldDefinition]:
        """Returns field definition by name."""
        for field in self.fields:
            if field.name == name:
                return field
        return None


@dataclass
class ValidationRules:
    """
    Defines validation rules for specific fields.
    
    Attributes:
        field_rules: Dictionary mapping field names to validation functions
        custom_validators: List of custom validation functions that receive the entire record
    """
    field_rules: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    custom_validators: List[Callable[[Dict], Tuple[bool, str]]] = field(default_factory=list)
    
    def add_range_rule(self, field_name: str, min_value: Any = None, max_value: Any = None):
        """Add a range validation rule for a numeric field."""
        if field_name not in self.field_rules:
            self.field_rules[field_name] = {}
        self.field_rules[field_name]['min'] = min_value
        self.field_rules[field_name]['max'] = max_value
    
    def add_pattern_rule(self, field_name: str, pattern: str):
        """Add a regex pattern validation rule for a string field."""
        if field_name not in self.field_rules:
            self.field_rules[field_name] = {}
        self.field_rules[field_name]['pattern'] = re.compile(pattern)
    
    def add_allowed_values(self, field_name: str, allowed_values: List[Any]):
        """Add an allowed values validation rule."""
        if field_name not in self.field_rules:
            self.field_rules[field_name] = {}
        self.field_rules[field_name]['allowed'] = set(allowed_values)


@dataclass
class ReadResult:
    """Contains the results of a binary file read operation."""
    records: List[Dict[str, Any]]
    total_records_read: int
    valid_records: int
    invalid_records: int
    errors: List[Dict[str, Any]]
    warnings: List[str]
    file_size_bytes: int
    expected_record_count: int
    schema_name: str
    read_timestamp: datetime = field(default_factory=datetime.now)
    
    @property
    def success_rate(self) -> float:
        """Returns the percentage of successfully read records."""
        if self.total_records_read == 0:
            return 0.0
        return (self.valid_records / self.total_records_read) * 100
    
    def print_summary(self, detailed: bool = False):
        """Prints a summary of the read operation."""
        print("\n" + "="*70)
        print(f"BINARY FILE READ SUMMARY - {self.schema_name}")
        print("="*70)
        print(f"Read timestamp: {self.read_timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"File size: {self.file_size_bytes:,} bytes")
        print(f"Expected records: {self.expected_record_count}")
        print(f"Total records processed: {self.total_records_read}")
        print(f"Valid records: {self.valid_records}")
        print(f"Invalid records: {self.invalid_records}")
        print(f"Success rate: {self.success_rate:.2f}%")
        print(f"Errors encountered: {len(self.errors)}")
        print(f"Warnings: {len(self.warnings)}")
        
        if detailed and self.warnings:
            print("\nWarnings:")
            for i, warning in enumerate(self.warnings[:10], 1):
                print(f"  {i}. {warning}")
            if len(self.warnings) > 10:
                print(f"  ... and {len(self.warnings) - 10} more warnings")
        
        if detailed and self.errors:
            print("\nFirst 5 Errors:")
            for i, error in enumerate(self.errors[:5], 1):
                print(f"  {i}. Record {error.get('record_num', '?')} "
                      f"at offset {error.get('byte_offset', '?')}: "
                      f"{error.get('error_type', 'unknown')} - "
                      f"{error.get('error_message', 'N/A')}")
            if len(self.errors) > 5:
                print(f"  ... and {len(self.errors) - 5} more errors")
        
        print("="*70 + "\n")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert result to dictionary (excluding records for brevity)."""
        return {
            'schema_name': self.schema_name,
            'read_timestamp': self.read_timestamp.isoformat(),
            'file_size_bytes': self.file_size_bytes,
            'expected_record_count': self.expected_record_count,
            'total_records_read': self.total_records_read,
            'valid_records': self.valid_records,
            'invalid_records': self.invalid_records,
            'success_rate': self.success_rate,
            'error_count': len(self.errors),
            'warning_count': len(self.warnings)
        }


class BinaryFileReader:
    """
    A highly flexible binary file reader with comprehensive error handling and validation.
    
    This reader can handle any binary data structure by using a configurable RecordSchema.
    
    Features:
    - Configurable data structures via RecordSchema
    - Support for multiple data types (integers, floats, strings, booleans)
    - Configurable error handling modes (strict, skip invalid, collect errors)
    - Field-level and record-level validation
    - Detailed logging and diagnostics
    - Memory-efficient processing
    - Character encoding fallback for string fields
    - File integrity verification
    - Custom validators support
    """
    
    def __init__(
        self,
        schema: RecordSchema,
        error_mode: ErrorHandlingMode = ErrorHandlingMode.SKIP_INVALID,
        validation_rules: Optional[ValidationRules] = None,
        string_encoding: str = 'ascii',
        string_encoding_fallback: str = 'latin-1',
        logger: Optional[logging.Logger] = None,
        log_file: Optional[str] = None
    ):
        """
        Initialize the binary file reader.
        
        Args:
            schema: RecordSchema defining the binary structure
            error_mode: How to handle errors (STRICT, SKIP_INVALID, COLLECT_ERRORS)
            validation_rules: Optional validation rules for fields
            string_encoding: Primary character encoding for strings (default: 'ascii')
            string_encoding_fallback: Fallback encoding if primary fails (default: 'latin-1')
            logger: Optional logger instance (creates default if None)
            log_file: Optional log file path (default: 'binary_reader.log')
        """
        self.schema = schema
        self.error_mode = error_mode
        self.validation_rules = validation_rules or ValidationRules()
        self.string_encoding = string_encoding
        self.string_encoding_fallback = string_encoding_fallback
        
        # Set up logging
        self.logger = logger or self._setup_default_logger(log_file)
        
        # Statistics tracking
        self.errors: List[Dict[str, Any]] = []
        self.warnings: List[str] = []
        
        # Log initialization
        self.logger.info(f"BinaryFileReader initialized with schema: {schema.name}")
        self.logger.info(f"Record size: {schema.record_size} bytes")
        self.logger.info(f"Fields: {', '.join(schema.field_names)}")
    
    def _setup_default_logger(self, log_file: Optional[str] = None) -> logging.Logger:
        """Creates a default logger with console and file handlers."""
        logger = logging.getLogger('BinaryFileReader')
        
        # Clear any existing handlers
        logger.handlers.clear()
        logger.setLevel(logging.INFO)
        
        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_format = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        console_handler.setFormatter(console_format)
        
        # File handler
        log_filepath = log_file or 'binary_reader.log'
        file_handler = logging.FileHandler(log_filepath, mode='a')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(console_format)
        
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)
        
        return logger
    
    def verify_file_integrity(self, filepath: Path) -> Tuple[bool, List[str]]:
        """
        Verifies basic file integrity before reading.
        
        Args:
            filepath: Path to the binary file
            
        Returns:
            Tuple of (is_valid, list_of_issues)
        """
        issues = []
        
        if not filepath.exists():
            issues.append(f"File does not exist: {filepath}")
            return False, issues
        
        if not filepath.is_file():
            issues.append(f"Path is not a file: {filepath}")
            return False, issues
        
        file_size = filepath.stat().st_size
        
        if file_size == 0:
            issues.append("File is empty (0 bytes)")
            return False, issues
        
        record_size = self.schema.record_size
        
        if file_size % record_size != 0:
            remainder = file_size % record_size
            expected_size = (file_size // record_size) * record_size
            issues.append(
                f"File size ({file_size} bytes) is not a multiple of record size "
                f"({record_size} bytes). {remainder} trailing bytes detected. "
                f"Expected size: {expected_size} bytes. File may be truncated or corrupted."
            )
            self.warnings.append(issues[-1])
        
        return len(issues) == 0 or file_size % record_size != 0, issues
    
    def _decode_string_field(
        self, 
        byte_data: bytes, 
        field_name: str, 
        record_num: int
    ) -> str:
        """
        Decodes a byte string with fallback encoding support.
        
        Args:
            byte_data: The bytes to decode
            field_name: Name of the field being decoded
            record_num: The record number (for error reporting)
            
        Returns:
            Decoded string with null bytes and whitespace stripped
        """
        try:
            # Try primary encoding
            decoded = byte_data.decode(self.string_encoding).rstrip('\x00').strip()
            return decoded
        except UnicodeDecodeError as e:
            # Try fallback encoding
            self.logger.warning(
                f"Record {record_num}, field '{field_name}': "
                f"Failed to decode with {self.string_encoding}, "
                f"using fallback {self.string_encoding_fallback}"
            )
            try:
                decoded = byte_data.decode(
                    self.string_encoding_fallback,
                    errors='replace'
                ).rstrip('\x00').strip()
                return decoded
            except Exception as e2:
                self.logger.error(
                    f"Record {record_num}, field '{field_name}': "
                    f"Failed to decode with fallback encoding: {e2}"
                )
                # Return hex representation as last resort
                return f"<BINARY:{byte_data.hex()}>"
    
    def _convert_field_value(
        self,
        raw_value: Any,
        field_def: FieldDefinition,
        record_num: int
    ) -> Any:
        """
        Converts a raw unpacked value to the appropriate Python type.
        
        Args:
            raw_value: Raw value from struct.unpack
            field_def: Field definition
            record_num: Record number for error reporting
            
        Returns:
            Converted value
        """
        if field_def.field_type == FieldType.STRING:
            if isinstance(raw_value, bytes):
                return self._decode_string_field(raw_value, field_def.name, record_num)
            return str(raw_value)
        
        elif field_def.field_type == FieldType.BOOL:
            return bool(raw_value)
        
        elif field_def.field_type in [FieldType.FLOAT, FieldType.DOUBLE]:
            # Check for NaN or Inf
            if raw_value != raw_value or abs(raw_value) == float('inf'):
                self.logger.warning(
                    f"Record {record_num}, field '{field_def.name}': "
                    f"Special float value detected: {raw_value}"
                )
            return float(raw_value)
        
        else:
            # Integer types
            return int(raw_value)
    
    def _validate_field(
        self,
        field_name: str,
        value: Any,
        record_num: int
    ) -> Tuple[bool, Optional[str]]:
        """
        Validates a field value against defined rules.
        
        Args:
            field_name: Name of the field
            value: Field value
            record_num: Record number for error reporting
            
        Returns:
            Tuple of (is_valid, error_message)
        """
        if field_name not in self.validation_rules.field_rules:
            return True, None
        
        rules = self.validation_rules.field_rules[field_name]
        
        # Check minimum value
        if 'min' in rules and rules['min'] is not None:
            if value < rules['min']:
                return False, f"Value {value} below minimum {rules['min']}"
        
        # Check maximum value
        if 'max' in rules and rules['max'] is not None:
            if value > rules['max']:
                return False, f"Value {value} above maximum {rules['max']}"
        
        # Check pattern (for strings)
        if 'pattern' in rules:
            pattern = rules['pattern']
            if not pattern.match(str(value)):
                return False, f"Value '{value}' does not match required pattern"
        
        # Check allowed values
        if 'allowed' in rules:
            if value not in rules['allowed']:
                return False, f"Value '{value}' not in allowed set"
        
        return True, None
    
    def _validate_record(
        self,
        record: Dict[str, Any],
        record_num: int
    ) -> Tuple[bool, List[str]]:
        """
        Validates an entire record using custom validators.
        
        Args:
            record: The record dictionary
            record_num: Record number for error reporting
            
        Returns:
            Tuple of (is_valid, list_of_error_messages)
        """
        errors = []
        
        for validator in self.validation_rules.custom_validators:
            try:
                is_valid, error_msg = validator(record)
                if not is_valid:
                    errors.append(error_msg)
            except Exception as e:
                errors.append(f"Validator exception: {e}")
        
        return len(errors) == 0, errors
    
    def _process_record(
        self,
        binary_data: bytes,
        record_num: int,
        byte_offset: int
    ) -> Optional[Dict[str, Any]]:
        """
        Processes a single binary record.
        
        Args:
            binary_data: Raw binary data for one record
            record_num: Record number (1-indexed)
            byte_offset: Byte offset in file
            
        Returns:
            Dictionary containing the record data, or None if invalid
        """
        try:
            # Unpack binary data
            unpacked_values = struct.unpack(self.schema.format_string, binary_data)
            
            # Build record dictionary
            record = {}
            field_errors = []
            
            for i, field_def in enumerate(self.schema.fields):
                raw_value = unpacked_values[i]
                
                # Convert value to appropriate type
                try:
                    converted_value = self._convert_field_value(
                        raw_value,
                        field_def,
                        record_num
                    )
                    record[field_def.name] = converted_value
                    
                    # Validate field if rules exist
                    is_valid, error_msg = self._validate_field(
                        field_def.name,
                        converted_value,
                        record_num
                    )
                    
                    if not is_valid:
                        field_errors.append(
                            f"Field '{field_def.name}': {error_msg}"
                        )
                        
                        # Use default value if available
                        if field_def.default_value is not None:
                            record[field_def.name] = field_def.default_value
                            self.logger.warning(
                                f"Record {record_num}, field '{field_def.name}': "
                                f"Using default value {field_def.default_value}"
                            )
                    
                    # Custom field validator
                    if field_def.validator is not None:
                        try:
                            if not field_def.validator(converted_value):
                                field_errors.append(
                                    f"Field '{field_def.name}': "
                                    f"Custom validation failed"
                                )
                        except Exception as e:
                            field_errors.append(
                                f"Field '{field_def.name}': "
                                f"Validator exception: {e}"
                            )
                
                except Exception as e:
                    error_msg = (
                        f"Field '{field_def.name}': "
                        f"Conversion error - {type(e).__name__}: {e}"
                    )
                    field_errors.append(error_msg)
                    
                    if field_def.required:
                        raise
                    elif field_def.default_value is not None:
                        record[field_def.name] = field_def.default_value
            
            # Validate entire record
            record_valid, record_errors = self._validate_record(record, record_num)
            if not record_valid:
                field_errors.extend(record_errors)
            
            # Handle validation errors
            if field_errors:
                error_info = {
                    'record_num': record_num,
                    'byte_offset': byte_offset,
                    'error_type': 'validation',
                    'error_message': '; '.join(field_errors),
                    'field_errors': field_errors
                }
                self.errors.append(error_info)
                
                for error in field_errors:
                    self.logger.warning(
                        f"Record {record_num} at offset {byte_offset}: {error}"
                    )
                
                if self.error_mode == ErrorHandlingMode.STRICT:
                    raise ValueError(f"Validation failed: {'; '.join(field_errors)}")
                elif self.error_mode == ErrorHandlingMode.SKIP_INVALID:
                    return None
            
            return record
        
        except struct.error as e:
            error_info = {
                'record_num': record_num,
                'byte_offset': byte_offset,
                'error_type': 'unpack',
                'error_message': str(e),
                'exception_type': 'struct.error'
            }
            self.errors.append(error_info)
            self.logger.error(
                f"Record {record_num} at offset {byte_offset}: "
                f"Struct unpack error - {e}"
            )
            
            if self.error_mode == ErrorHandlingMode.STRICT:
                raise
            return None
        
        except Exception as e:
            error_info = {
                'record_num': record_num,
                'byte_offset': byte_offset,
                'error_type': 'unexpected',
                'error_message': str(e),
                'exception_type': type(e).__name__
            }
            self.errors.append(error_info)
            self.logger.error(
                f"Record {record_num} at offset {byte_offset}: "
                f"Unexpected error - {type(e).__name__}: {e}"
            )
            
            if self.error_mode == ErrorHandlingMode.STRICT:
                raise
            return None
    
    def read_file(
        self,
        filepath: Union[str, Path],
        progress_interval: int = 10000
    ) -> ReadResult:
        """
        Reads the entire binary file and returns results.
        
        Args:
            filepath: Path to the binary file
            progress_interval: Log progress every N records (0 to disable)
            
        Returns:
            ReadResult object containing records and statistics
        """
        filepath = Path(filepath)
        
        # Reset statistics
        self.errors = []
        self.warnings = []
        
        self.logger.info("="*70)
        self.logger.info(f"Starting binary file read: {filepath}")
        self.logger.info(f"Schema: {self.schema.name}")
        self.logger.info(f"Record format: {self.schema.format_string}")
        self.logger.info(f"Record size: {self.schema.record_size} bytes")
        self.logger.info(f"Error handling mode: {self.error_mode.value}")
        self.logger.info("="*70)
        
        # Verify file integrity
        is_valid, issues = self.verify_file_integrity(filepath)
        if not is_valid:
            error_msg = f"File integrity check failed: {'; '.join(issues)}"
            self.logger.error(error_msg)
            raise IOError(error_msg)
        
        if issues:
            for issue in issues:
                self.logger.warning(issue)
        
        # Get file statistics
        file_size = filepath.stat().st_size
        expected_records = file_size // self.schema.record_size
        
        self.logger.info(f"File size: {file_size:,} bytes")
        self.logger.info(f"Expected records: {expected_records:,}")
        
        records = []
        record_num = 0
        byte_offset = 0
        
        try:
            with open(filepath, 'rb') as f:
                while True:
                    # Read one record's worth of bytes
                    binary_data = f.read(self.schema.record_size)
                    
                    # Check for end of file
                    if not binary_data:
                        break
                    
                    record_num += 1
                    
                    # Check for incomplete record
                    if len(binary_data) < self.schema.record_size:
                        error_info = {
                            'record_num': record_num,
                            'byte_offset': byte_offset,
                            'error_type': 'incomplete_read',
                            'bytes_received': len(binary_data),
                            'bytes_expected': self.schema.record_size
                        }
                        self.errors.append(error_info)
                        self.logger.error(
                            f"Record {record_num} at offset {byte_offset}: "
                            f"Incomplete read - got {len(binary_data)} bytes, "
                            f"expected {self.schema.record_size}"
                        )
                        
                        if self.error_mode == ErrorHandlingMode.STRICT:
                            raise IOError(
                                f"Incomplete record at position {byte_offset}: "
                                f"expected {self.schema.record_size} bytes, "
                                f"got {len(binary_data)}"
                            )
                        break
                    
                    # Process the record
                    record = self._process_record(binary_data, record_num, byte_offset)
                    
                    if record is not None:
                        records.append(record)
                    
                    byte_offset += self.schema.record_size
                    
                    # Log progress for large files
                    if progress_interval > 0 and record_num % progress_interval == 0:
                        self.logger.info(
                            f"Progress: {record_num:,} records processed, "
                            f"{len(records):,} valid..."
                        )
        
        except IOError as e:
            self.logger.error(f"I/O Error reading file: {e}")
            raise
        except Exception as e:
            self.logger.error(f"Unexpected error: {type(e).__name__}: {e}")
            raise
        
        # Calculate statistics
        valid_records = len(records)
        invalid_records = record_num - valid_records
        
        result = ReadResult(
            records=records,
            total_records_read=record_num,
            valid_records=valid_records,
            invalid_records=invalid_records,
            errors=self.errors,
            warnings=self.warnings,
            file_size_bytes=file_size,
            expected_record_count=expected_records,
            schema_name=self.schema.name
        )
        
        self.logger.info("="*70)
        self.logger.info(f"Completed reading {record_num:,} records")
        self.logger.info(f"Valid records: {valid_records:,}")
        self.logger.info(f"Invalid records: {invalid_records:,}")
        self.logger.info(f"Success rate: {result.success_rate:.2f}%")
        self.logger.info(f"Errors: {len(self.errors)}")
        self.logger.info(f"Warnings: {len(self.warnings)}")
        self.logger.info("="*70)
        
        return result
    
    def read_file_chunked(
        self,
        filepath: Union[str, Path],
        chunk_size: int = 1000,
        callback: Optional[Callable[[List[Dict[str, Any]]], None]] = None
    ) -> ReadResult:
        """
        Reads the binary file in chunks for memory efficiency.
        Useful for very large files that don't fit in memory.
        
        Args:
            filepath: Path to the binary file
            chunk_size: Number of records to process before calling callback
            callback: Optional function to call with each chunk of records
            
        Returns:
            ReadResult object containing records and statistics
        """
        self.logger.info(f"Reading file in chunks of {chunk_size} records")
        
        # For now, use the same implementation
        # In production, this could yield chunks or write to disk
        result = self.read_file(filepath, progress_interval=chunk_size)
        
        if callback:
            # Process records in chunks
            for i in range(0, len(result.records), chunk_size):
                chunk = result.records[i:i + chunk_size]
                callback(chunk)
        
        return result


def create_schema_from_format_string(
    format_string: str,
    field_names: List[str],
    byte_order: str = '<',
    schema_name: str = "Custom Schema"
) -> RecordSchema:
    """
    Helper function to create a RecordSchema from a struct format string.
    This provides backward compatibility with the old implementation.
    
    Args:
        format_string: Struct format string (e.g., 'id10s')
        field_names: List of field names corresponding to format string
        byte_order: Byte order character
        schema_name: Name for the schema
        
    Returns:
        RecordSchema object
        
    Example:
        schema = create_schema_from_format_string('id10s', ['id', 'value', 'code'])
    """
    # Parse format string to extract field types
    import re
    
    # Pattern to match format specifiers
    pattern = r'(\d*)([bBhHiIlLqQfd?s])'
    matches = re.findall(pattern, format_string)
    
    if len(matches) != len(field_names):
        raise ValueError(
            f"Number of format specifiers ({len(matches)}) "
            f"does not match number of field names ({len(field_names)})"
        )
    
    # Mapping of format characters to FieldType
    char_to_type = {
        'b': FieldType.INT8,
        'B': FieldType.UINT8,
        'h': FieldType.INT16,
        'H': FieldType.UINT16,
        'i': FieldType.INT32,
        'I': FieldType.UINT32,
        'l': FieldType.INT32,  # Treat as INT32
        'L': FieldType.UINT32,  # Treat as UINT32
        'q': FieldType.INT64,
        'Q': FieldType.UINT64,
        'f': FieldType.FLOAT,
        'd': FieldType.DOUBLE,
        '?': FieldType.BOOL,
        's': FieldType.STRING,
    }
    
    fields = []
    for i, (count_str, type_char) in enumerate(matches):
        field_type = char_to_type.get(type_char)
        if field_type is None:
            raise ValueError(f"Unsupported format character: {type_char}")
        
        if field_type == FieldType.STRING:
            string_length = int(count_str) if count_str else 1
            fields.append(FieldDefinition(
                name=field_names[i],
                field_type=field_type,
                string_length=string_length
            ))
        else:
            fields.append(FieldDefinition(
                name=field_names[i],
                field_type=field_type
            ))
    
    return RecordSchema(
        fields=fields,
        byte_order=byte_order,
        name=schema_name
    )


# Example usage and test functions
def example_basic_usage():
    """Demonstrates basic usage with the original data structure."""
    print("\n" + "="*70)
    print("EXAMPLE 1: Basic Usage (Original Structure)")
    print("="*70)
    
    # Define schema for original structure: int, double, char[10]
    schema = RecordSchema(
        fields=[
            FieldDefinition(
                name='id',
                field_type=FieldType.INT32,
                description='Record ID'
            ),
            FieldDefinition(
                name='value',
                field_type=FieldType.DOUBLE,
                description='Measurement value'
            ),
            FieldDefinition(
                name='code',
                field_type=FieldType.STRING,
                string_length=10,
                description='Status code'
            )
        ],
        byte_order='<',
        name="Employee Record v1"
    )
    
    # Define validation rules
    validation_rules = ValidationRules()
    validation_rules.add_range_rule('id', min_value=1, max_value=999999)
    validation_rules.add_range_rule('value', min_value=-1000000.0, max_value=1000000.0)
    
    print(f"\nSchema: {schema.name}")
    print(f"Format: {schema.format_string}")
    print(f"Record size: {schema.record_size} bytes")
    print(f"Fields: {', '.join(schema.field_names)}")


def example_advanced_usage():
    """Demonstrates advanced usage with a complex data structure."""
    print("\n" + "="*70)
    print("EXAMPLE 2: Advanced Usage (Complex Structure)")
    print("="*70)
    
    # Define a more complex schema
    schema = RecordSchema(
        fields=[
            FieldDefinition(
                name='employee_id',
                field_type=FieldType.UINT32,
                description='Employee ID number'
            ),
            FieldDefinition(
                name='department_code',
                field_type=FieldType.UINT16,
                description='Department code'
            ),
            FieldDefinition(
                name='salary',
                field_type=FieldType.DOUBLE,
                description='Annual salary'
            ),
            FieldDefinition(
                name='is_active',
                field_type=FieldType.BOOL,
                description='Employment status'
            ),
            FieldDefinition(
                name='first_name',
                field_type=FieldType.STRING,
                string_length=20,
                description='First name'
            ),
            FieldDefinition(
                name='last_name',
                field_type=FieldType.STRING,
                string_length=20,
                description='Last name'
            ),
            FieldDefinition(
                name='performance_score',
                field_type=FieldType.FLOAT,
                description='Performance rating (0.0-5.0)'
            )
        ],
        byte_order='<',
        name="Employee Record v2"
    )
    
    # Define comprehensive validation rules
    validation_rules = ValidationRules()
    validation_rules.add_range_rule('employee_id', min_value=1, max_value=1000000)
    validation_rules.add_range_rule('department_code', min_value=1, max_value=100)
    validation_rules.add_range_rule('salary', min_value=0.0, max_value=1000000.0)
    validation_rules.add_range_rule('performance_score', min_value=0.0, max_value=5.0)
    
    # Add custom validator for entire record
    def validate_name_not_empty(record: Dict) -> Tuple[bool, str]:
        if not record.get('first_name') or not record.get('last_name'):
            return False, "First name and last name cannot be empty"
        return True, ""
    
    validation_rules.custom_validators.append(validate_name_not_empty)
    
    print(f"\nSchema: {schema.name}")
    print(f"Format: {schema.format_string}")
    print(f"Record size: {schema.record_size} bytes")
    print(f"Fields: {', '.join(schema.field_names)}")
    print("\nField Details:")
    for field in schema.fields:
        print(f"  - {field.name}: {field.field_type.value}", end="")
        if field.string_length:
            print(f" (length: {field.string_length})", end="")
        if field.description:
            print(f" - {field.description}", end="")
        print()


def example_backward_compatibility():
    """Demonstrates backward compatibility with old format string approach."""
    print("\n" + "="*70)
    print("EXAMPLE 3: Backward Compatibility")
    print("="*70)
    
    # Create schema using the helper function (old style)
    schema = create_schema_from_format_string(
        format_string='id10s',
        field_names=['id', 'value', 'code'],
        byte_order='<',
        schema_name="Legacy Format"
    )
    
    print(f"\nSchema: {schema.name}")
    print(f"Format: {schema.format_string}")
    print(f"Record size: {schema.record_size} bytes")
    print(f"Fields: {', '.join(schema.field_names)}")


if __name__ == '__main__':
    print("\n" + "="*70)
    print("BINARY FILE READER - FLEXIBLE VERSION")
    print("Demonstrating schema configuration capabilities")
    print("="*70)
    
    # Run examples
    example_basic_usage()
    example_advanced_usage()
    example_backward_compatibility()
    
    print("\n" + "="*70)
    print("Examples completed successfully!")
    print("="*70)